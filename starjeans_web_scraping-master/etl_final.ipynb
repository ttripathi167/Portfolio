{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180b422a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82487e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:44.938995Z",
     "start_time": "2022-06-14T21:40:44.504934Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sqlite3\n",
    "import requests\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "\n",
    "from datetime   import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from bs4        import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b54060",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db0e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:45.722161Z",
     "start_time": "2022-06-14T21:40:44.946236Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "# URL\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "# Request to URL\n",
    "page = requests.get( url, headers=headers )\n",
    "\n",
    "# Beautiful soup object\n",
    "soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "# ==================== pagination ======================================\n",
    "total_item = soup.find_all( 'h2', class_='load-more-heading' )[0].get('data-total')\n",
    "page_number = math.ceil( int( total_item ) / 36)\n",
    "url02 = url + '?page-size=' + str( int( page_number*36 ))\n",
    "page = requests.get( url02, headers=headers )\n",
    "soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "# ======================= Product Data =======================\n",
    "products = soup.find( 'ul', class_='products-listing small' ) \n",
    "product_list = products.find_all( 'article', class_='hm-product-item')\n",
    "\n",
    "# product id\n",
    "product_id = [p.get( 'data-articlecode' ) for p in product_list]\n",
    "\n",
    "# product category\n",
    "product_category = [p.get( 'data-category' ) for p in product_list]\n",
    "\n",
    "# product name\n",
    "product_list = products.find_all( 'a', class_='link' )\n",
    "product_name = [p.get_text() for p in product_list]\n",
    "\n",
    "# product_price\n",
    "product_list = products.find_all('span', class_='price regular')\n",
    "product_price = [p.get_text() for p in product_list]\n",
    "\n",
    "# create data frame\n",
    "data = pd.DataFrame( [product_id, product_name, product_price, product_category] ).T\n",
    "data.columns = ['product_id', 'product_name', 'price', 'product_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e37bf3",
   "metadata": {},
   "source": [
    "## Data Collection by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fc2ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.813919Z",
     "start_time": "2022-06-14T21:40:45.733419Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# empty dataframe\n",
    "df_compositions = pd.DataFrame()\n",
    "\n",
    "# unique columns for all products\n",
    "aux = []\n",
    "\n",
    "df_pattern = pd.DataFrame( columns = ['Art. No.', 'Composition', 'Fit', 'Size'] )\n",
    "for i in range ( len( data ) ):\n",
    "\n",
    "    # API Requests\n",
    "    url = 'https://www2.hm.com/en_us/productpage.' + data.loc[i, 'product_id'] + '.html'\n",
    "    \n",
    "    page = requests.get( url, headers=headers )\n",
    "\n",
    "    # Beautiful Soup object\n",
    "    soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "\n",
    "# ==================== color name ==================================\n",
    "    product_list = soup.find_all( 'a', class_='filter-option miniature active') + soup.find_all( 'a', class_='filter-option miniature')\n",
    "    color_name = [p.get('data-color') for p in product_list]\n",
    "\n",
    "    # product id\n",
    "    product_id = [p.get( 'data-articlecode' ) for p in product_list]\n",
    "\n",
    "    df_color = pd.DataFrame( [product_id, color_name] ).T\n",
    "    df_color.columns = ['product_id', 'color_name']\n",
    "    \n",
    "    for j in range( len( df_color ) ):\n",
    "        # API Requests\n",
    "        url = 'https://www2.hm.com/en_us/productpage.' + df_color.loc[j, 'product_id'] + '.html'\n",
    "\n",
    "        page = requests.get( url, headers=headers )\n",
    "\n",
    "        # Beautiful Soup object\n",
    "        soup = BeautifulSoup( page.text, 'html.parser' )\n",
    "        \n",
    "        # ====================== Product Name =================================\n",
    "        product_name = soup.find_all( 'h1', class_='primary product-item-headline')\n",
    "        product_name = product_name[0].get_text()\n",
    "        \n",
    "        # ====================== Product Price =================================\n",
    "        product_price = soup.find_all( 'div', class_='primary-row product-item-price')\n",
    "        product_price = re.findall('\\d+\\.?\\d+', product_price[0].get_text())[0]\n",
    "        \n",
    "        \n",
    "        # ==================== composition ==================================\n",
    "        product_composition_list = soup.find_all( 'div', class_='details-attributes-list-item' )\n",
    "        product_composition =[list( filter( None, p.get_text().split( '\\n' ) ) ) for p in product_composition_list]\n",
    "\n",
    "        # rename dataframe\n",
    "        df_composition = pd.DataFrame(product_composition).T\n",
    "        df_composition.columns = df_composition.iloc[0]\n",
    "\n",
    "        # delete first row\n",
    "        df_composition = df_composition.iloc[1:].fillna( method='ffill' )\n",
    "\n",
    "        # remove pocket lining, shell, pocket and lining\n",
    "        df_composition['Composition'] = df_composition['Composition'].str.replace( 'Pocket lining: ', '', regex=True )\n",
    "        df_composition['Composition'] = df_composition['Composition'].str.replace( 'Shell: ', '', regex=True )\n",
    "        df_composition['Composition'] = df_composition['Composition'].str.replace( 'Lining: ', '', regex=True )\n",
    "        df_composition['Composition'] = df_composition['Composition'].str.replace( 'Pocket: ', '', regex=True )\n",
    "\n",
    "        # garantee the same number of columns\n",
    "        df_composition = pd.concat( [df_pattern, df_composition] )\n",
    "\n",
    "        # rename columns\n",
    "        df_composition = df_composition[['Art. No.', 'Composition', 'Fit', 'Size']]\n",
    "        df_composition.columns = ['product_id', 'composition', 'fit', 'size']\n",
    "        df_composition['product_name'] = product_name\n",
    "        df_composition['product_price'] = product_price\n",
    "\n",
    "        # keep new columns if it show up\n",
    "        aux = aux + df_composition.columns.tolist()\n",
    "\n",
    "        # merge data color + composition\n",
    "        df_composition = pd.merge( df_composition, df_color, how='left', on='product_id')\n",
    "        \n",
    "        df_composition = df_composition.drop_duplicates()\n",
    "        \n",
    "        # all products\n",
    "        df_compositions = pd.concat( [df_compositions, df_composition], axis=0 )\n",
    "        \n",
    "    \n",
    "# generate style id + color id\n",
    "df_compositions['style_id'] = df_compositions['product_id'].apply( lambda x: x[:-3])\n",
    "df_compositions['color_id'] = df_compositions['product_id'].apply( lambda x: x[-3:])\n",
    "\n",
    "# scrapy datetime\n",
    "df_compositions['scrapy_datetime'] = datetime.now().strftime( '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760623f0",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dd0fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.818447Z",
     "start_time": "2022-06-14T21:40:46.818415Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# product id\n",
    "df_data = df_compositions.dropna( subset=['product_id'])\n",
    "\n",
    "# product_name\n",
    "df_data['product_name'] = df_data['product_name'].str.replace('\\n', '')\n",
    "df_data['product_name'] = df_data['product_name'].str.replace('\\t', '')\n",
    "df_data['product_name'] = df_data['product_name'].str.replace('  ', '')\n",
    "df_data['product_name'] = df_data['product_name'].str.replace(' ', '_').str.lower()\n",
    "\n",
    "# product_price\n",
    "df_data['product_price'] = df_data['product_price'].astype( float )\n",
    "\n",
    "# color_name\n",
    "df_data['color_name'] = df_data['color_name'].str.replace( ' ', '_' ).str.lower ()\n",
    "\n",
    "# fit\n",
    "df_data['fit'] = df_data['fit'].apply( lambda x: x.replace( ' ', '_' ).lower() if pd.notnull( x ) else x)\n",
    "\n",
    "# size leg\n",
    "df_data['size_leg'] = df_data['size'].apply( lambda x: re.search( '(\\d{2}\\.\\d) cm', x ).group(1) if pd.notnull(x) else x)\n",
    "\n",
    "# size number\n",
    "df_data['size_number'] = df_data['size'].apply( lambda x: re.search('Size (.+)', x).group(1).replace( ')', '') if pd.notnull(x) else x)\n",
    "\n",
    "# ============================ composition ===========================================\n",
    "# break composition by comma\n",
    "df1 = df_data['composition'].str.split(',', expand=True).reset_index(drop=True)\n",
    "\n",
    "# coton | polyester | spandex\n",
    "df_ref = pd.DataFrame(index=np.arange(len(df_data)), columns=['cotton', 'polyester', 'spandex'])\n",
    "\n",
    "# ------------------- coton -------------------\n",
    "df_cotton_0 = df1.loc[df1[0].str.contains( 'Cotton', na=True), 0]\n",
    "df_cotton_0.name = 'cotton'\n",
    "\n",
    "df_cotton_1 = df1.loc[df1[1].str.contains( 'Cotton', na=True), 1]\n",
    "df_cotton_1.name = 'cotton'\n",
    "\n",
    "# combine\n",
    "df_cotton = df_cotton_0.combine_first( df_cotton_1)\n",
    "\n",
    "df_ref = pd.concat( [df_ref, df_cotton], axis=1 )\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated( keep='last')]\n",
    "\n",
    "# ------------------- polyester -------------------\n",
    "df_polyester_0 = df1.loc[df1[0].str.contains( 'Polyester', na=True), 0]\n",
    "df_polyester_0.name = 'polyester'\n",
    "\n",
    "df_polyester_1 = df1.loc[df1[1].str.contains( 'Polyester', na=True), 1]\n",
    "df_polyester_1.name = 'polyester'\n",
    "\n",
    "# combine\n",
    "df_polyester = df_polyester_0.combine_first( df_polyester_1 )\n",
    "\n",
    "df_ref = pd.concat( [df_ref, df_polyester], axis=1 )\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated( keep='last')]\n",
    "\n",
    "# ------------------- spandex -------------------\n",
    "df_spandex_1 = df1.loc[df1[1].str.contains( 'Spandex', na=True), 1]\n",
    "df_spandex_1.name = 'spandex'\n",
    "\n",
    "df_spandex_2 = df1.loc[df1[2].str.contains( 'Spandex', na=True), 2]\n",
    "df_spandex_2.name = 'spandex'\n",
    "\n",
    "df_spandex_3 = df1.loc[df1[3].str.contains( 'Spandex', na=True), 3]\n",
    "df_spandex_3.name = 'spandex'\n",
    "\n",
    "# combine spandex from both columns 1 and 2\n",
    "df_spandex_c2 = df_spandex_1.combine_first( df_spandex_2)\n",
    "df_spandex = df_spandex_c2.combine_first( df_spandex_3)\n",
    "\n",
    "df_ref = pd.concat( [df_ref, df_spandex], axis=1 )\n",
    "df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated( keep='last')]\n",
    "\n",
    "# join of combine with product_id\n",
    "df_aux = pd.concat( [df_data['product_id'].reset_index(drop=True),df_ref], axis=1)\n",
    "\n",
    "# format composition df_data\n",
    "df_aux['cotton'] = df_aux['cotton'].apply( lambda x: int( re.search('\\d+', x).group(0) ) / 100 if pd.notnull( x ) else x)\n",
    "df_aux['polyester'] = df_aux['polyester'].apply( lambda x: int( re.search('\\d+', x).group(0) ) / 100 if pd.notnull( x ) else x)\n",
    "df_aux['spandex'] = df_aux['spandex'].apply( lambda x: int( re.search('\\d+', x).group(0) ) / 100 if pd.notnull( x ) else x)\n",
    "\n",
    "# final join\n",
    "df_aux = df_aux.groupby( 'product_id' ).max().reset_index().fillna( 0 )\n",
    "\n",
    "df_data = pd.merge( df_data, df_aux, on='product_id', how='left' )\n",
    "\n",
    "# Drop columns\n",
    "df_data = df_data.drop( columns=['size', 'composition'], axis=1 )\n",
    "\n",
    "# Drop duplicates\n",
    "df_data = df_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4b723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.820442Z",
     "start_time": "2022-06-14T21:40:46.820418Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be034f",
   "metadata": {},
   "source": [
    "# Data Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67feb740",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.823159Z",
     "start_time": "2022-06-14T21:40:46.823132Z"
    }
   },
   "outputs": [],
   "source": [
    "data_insert = df_data[[\n",
    "    'product_id',\n",
    "    'style_id',\n",
    "    'color_id',\n",
    "    'product_name',\n",
    "    'color_name',\n",
    "    'fit',\n",
    "    'product_price',\n",
    "    'size_number',\n",
    "    'size_leg',\n",
    "    'cotton',\n",
    "    'polyester',\n",
    "    'spandex',\n",
    "    'scrapy_datetime'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd7151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.824700Z",
     "start_time": "2022-06-14T21:40:46.824676Z"
    }
   },
   "outputs": [],
   "source": [
    "query_showroom_schema = \"\"\"\n",
    "    CREATE TABLE vitrine(\n",
    "    product_id         TEXT,\n",
    "    style_id           TEXT,\n",
    "    color_id           TEXT,\n",
    "    product_name       TEXT,\n",
    "    color_name         TEXT,\n",
    "    fit                TEXT,\n",
    "    product_price      REAL,\n",
    "    size_number        TEXT,\n",
    "    size_leg           TEXT,\n",
    "    cotton             REAL,\n",
    "    polyester          REAL,\n",
    "    spandex            REAL,\n",
    "    scrapy_datetime    TEXT\n",
    "    )\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e624597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.826273Z",
     "start_time": "2022-06-14T21:40:46.826246Z"
    }
   },
   "outputs": [],
   "source": [
    "# create table\n",
    "conn = sqlite3.connect( 'database_hm.sqlite' )\n",
    "cursor = conn.execute( query_showroom_schema )\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0936e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T21:40:46.829200Z",
     "start_time": "2022-06-14T21:40:46.829183Z"
    }
   },
   "outputs": [],
   "source": [
    "# create database connection\n",
    "conn = create_engine( 'sqlite:///database_hm.sqlite', echo=False )\n",
    "\n",
    "# data insert\n",
    "data_insert.to_sql( 'vitrine', con=conn, if_exists='append', index=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
